{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,sampler,Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import timeit\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten3d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        ###############6th TODO (5 points)###################\n",
    "        N, P, C, H, W = x.size() \n",
    "        return x.view(N, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_model_3d_frame = nn.Sequential( # You fill this in!\n",
    "    ###############7th TODO (20 points)#########################\n",
    "    nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm3d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "    nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm3d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "    Flatten3d(),\n",
    "    nn.Linear(32768, 2048),   \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(2048, 10),\n",
    ")\n",
    "\n",
    "fixed_model_3d = fixed_model_3d_frame.type(dtype)\n",
    "x = torch.randn(32,3, 3, 64, 64).type(dtype)\n",
    "x_var = Variable(x).type(dtype) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_3d(x_var) \n",
    "np.array_equal(np.array(ans.size()), np.array([32, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "fixed_model_gpu = copy.deepcopy(fixed_model_3d)#.type(gpu_dtype)\n",
    "fixed_model_gpu.cuda()\n",
    "x_gpu = torch.randn(32,3,3,64,64).cuda()#.type(gpu_dtype)\n",
    "x_var_gpu = Variable(x_gpu).type(gpu_dtype) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_gpu(x_var_gpu)        # Feed it through the model! \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([32, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7770\n",
      "2230\n"
     ]
    }
   ],
   "source": [
    "label_mat=scipy.io.loadmat('./data/hw6_data.mat')\n",
    "label_train=label_mat['trLb']\n",
    "print(len(label_train))\n",
    "label_val=label_mat['valLb']\n",
    "print(len(label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cf5e1ade57b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m clip_dataloader = DataLoader(clip_dataset, batch_size=4,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         shuffle=True, num_workers=4)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "clip_dataloader = DataLoader(clip_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(clip_dataloader):\n",
    "    print(i,sample['clip'].shape,sample['folder'],sample['Label'])\n",
    "    if i>20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataset_train=ActionClipDataset(root_dir='./data/trainClips/',labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_train = DataLoader(clip_dataset_train, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "clip_dataset_val=ActionClipDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_val = DataLoader(clip_dataset_val, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "clip_dataset_test=ActionClipDataset(root_dir='./data/testClips/',labels=[],transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_test = DataLoader(clip_dataset_test, batch_size=16,\n",
    "                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor # the CPU datatype\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_3d(model, loss_fn, optimizer,dataloader,num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, sample in enumerate(dataloader):\n",
    "            x_var = Variable(sample['clip'].type(dtype))\n",
    "            y_var = Variable(sample['Label'].type(dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        check_accuracy_3d(model, clip_dataloader_train) \n",
    "        check_accuracy_3d(model, clip_dataloader_val) \n",
    "        torch.save(model.state_dict(), os.path.join('./', 'epoch-{}.pth'.format(epoch)))\n",
    "        \n",
    "\n",
    "def check_accuracy_3d(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['clip'].type(dtype))\n",
    "        y_var = sample['Label'].type(dtype)\n",
    "        y_var=y_var.cpu()\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        #print(preds)\n",
    "        #print(y_var)\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform_TJ = T.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00001\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00002\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00003\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00004\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00005\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00006\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00007\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00008\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00009\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00010\n"
     ]
    }
   ],
   "source": [
    "class ActionClipDataset(Dataset):\n",
    "    \"\"\"Action Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,  root_dir,labels=[], transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.length=len(os.listdir(self.root_dir))\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        folder=idx+1\n",
    "        folder=format(folder,'05d')\n",
    "        clip=[]\n",
    "        if len(self.labels)!=0:\n",
    "            Label=self.labels[idx][0]-1\n",
    "        for i in range(3):\n",
    "            imidx=i+1\n",
    "            imgname=str(imidx)+'.jpg'\n",
    "            img_path = os.path.join(self.root_dir,\n",
    "                                    folder,imgname)\n",
    "            image = Image.open(img_path)\n",
    "            image=np.array(image)\n",
    "            clip.append(image)\n",
    "        if self.transform:\n",
    "            clip=np.asarray(clip)\n",
    "            clip=np.transpose(clip, (0,3,1,2))\n",
    "            clip = torch.from_numpy(np.asarray(clip))\n",
    "        if len(self.labels)!=0:\n",
    "            sample={'clip':clip,'Label':Label,'folder':folder}\n",
    "        else:\n",
    "            sample={'clip':clip,'folder':folder}\n",
    "        return sample\n",
    "\n",
    "clip_dataset=ActionClipDataset(root_dir='./data/trainClips/',\\\n",
    "                               labels=label_train,transform=T.ToTensor())#/home/tqvinh/Study/CSE512/cse512-s18/hw2data/trainClips/\n",
    "for i in range(10):\n",
    "    sample=clip_dataset[i]\n",
    "    print(sample['clip'].shape)\n",
    "    print(sample['Label'])\n",
    "    print(sample['folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, torch.Size([4, 3, 3, 64, 64]), ['01670', '01838', '05799', '02410'], tensor([1., 2., 7., 2.], dtype=torch.float64))\n",
      "(1, torch.Size([4, 3, 3, 64, 64]), ['03730', '01299', '05095', '00772'], tensor([4., 1., 6., 0.], dtype=torch.float64))\n",
      "(2, torch.Size([4, 3, 3, 64, 64]), ['00960', '03267', '00615', '07076'], tensor([1., 3., 0., 8.], dtype=torch.float64))\n",
      "(3, torch.Size([4, 3, 3, 64, 64]), ['02341', '01900', '04565', '06235'], tensor([2., 2., 5., 7.], dtype=torch.float64))\n",
      "(4, torch.Size([4, 3, 3, 64, 64]), ['04065', '04808', '00025', '06827'], tensor([4., 5., 0., 8.], dtype=torch.float64))\n",
      "(5, torch.Size([4, 3, 3, 64, 64]), ['03175', '05810', '07177', '06202'], tensor([3., 7., 9., 7.], dtype=torch.float64))\n",
      "(6, torch.Size([4, 3, 3, 64, 64]), ['01894', '00710', '05611', '05043'], tensor([2., 0., 6., 6.], dtype=torch.float64))\n",
      "(7, torch.Size([4, 3, 3, 64, 64]), ['05726', '04530', '05464', '01105'], tensor([7., 5., 6., 1.], dtype=torch.float64))\n",
      "(8, torch.Size([4, 3, 3, 64, 64]), ['03187', '00188', '04500', '00184'], tensor([3., 0., 5., 0.], dtype=torch.float64))\n",
      "(9, torch.Size([4, 3, 3, 64, 64]), ['03495', '02357', '02562', '02715'], tensor([3., 2., 2., 3.], dtype=torch.float64))\n",
      "(10, torch.Size([4, 3, 3, 64, 64]), ['00792', '00750', '03486', '07582'], tensor([0., 0., 3., 9.], dtype=torch.float64))\n",
      "(11, torch.Size([4, 3, 3, 64, 64]), ['01175', '06421', '07136', '03011'], tensor([1., 7., 9., 3.], dtype=torch.float64))\n",
      "(12, torch.Size([4, 3, 3, 64, 64]), ['02287', '01892', '05832', '04300'], tensor([2., 2., 7., 5.], dtype=torch.float64))\n",
      "(13, torch.Size([4, 3, 3, 64, 64]), ['07577', '06514', '05212', '02848'], tensor([9., 8., 6., 3.], dtype=torch.float64))\n",
      "(14, torch.Size([4, 3, 3, 64, 64]), ['00019', '04511', '07199', '04599'], tensor([0., 5., 9., 5.], dtype=torch.float64))\n",
      "(15, torch.Size([4, 3, 3, 64, 64]), ['04231', '05064', '01910', '05127'], tensor([5., 6., 2., 6.], dtype=torch.float64))\n",
      "(16, torch.Size([4, 3, 3, 64, 64]), ['04887', '07212', '04626', '04670'], tensor([5., 9., 5., 5.], dtype=torch.float64))\n",
      "(17, torch.Size([4, 3, 3, 64, 64]), ['00189', '04436', '02022', '04265'], tensor([0., 5., 2., 5.], dtype=torch.float64))\n",
      "(18, torch.Size([4, 3, 3, 64, 64]), ['00635', '06306', '05834', '02025'], tensor([0., 7., 7., 2.], dtype=torch.float64))\n",
      "(19, torch.Size([4, 3, 3, 64, 64]), ['02252', '00729', '07237', '03205'], tensor([2., 0., 9., 3.], dtype=torch.float64))\n",
      "(20, torch.Size([4, 3, 3, 64, 64]), ['03659', '01117', '03969', '07732'], tensor([4., 1., 4., 9.], dtype=torch.float64))\n",
      "(21, torch.Size([4, 3, 3, 64, 64]), ['02770', '03784', '00438', '03109'], tensor([3., 4., 0., 3.], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "clip_dataloader = DataLoader(clip_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(clip_dataloader):\n",
    "    print(i,sample['clip'].shape,sample['folder'],sample['Label'])\n",
    "    if i>20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataset_train=ActionClipDataset(root_dir='./data/trainClips/',labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_train = DataLoader(clip_dataset_train, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "clip_dataset_val=ActionClipDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_val = DataLoader(clip_dataset_val, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "clip_dataset_test=ActionClipDataset(root_dir='./data/testClips/',labels=[],transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_test = DataLoader(clip_dataset_test, batch_size=16,\n",
    "                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten3d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        ###############6th TODO (5 points)###################\n",
    "        N, P, C, H, W = x.size() \n",
    "        return x.view(N, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_model_3d_frame = nn.Sequential( # You fill this in!\n",
    "    ###############7th TODO (20 points)#########################\n",
    "    nn.Conv3d(3, 8, kernel_size=(1,3,3), stride=1),\n",
    "    nn.BatchNorm3d(8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=(3, 2, 2), stride=(1,2,2), padding=1),\n",
    "    \n",
    "    # conv 2\n",
    "    nn.Conv3d(8, 16, kernel_size=(1,3,3), stride=1),\n",
    "    nn.BatchNorm3d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=(3, 2, 2), stride=(1,2,2), padding=1),\n",
    "     # conv 3\n",
    "    nn.Conv3d(16, 32, kernel_size=(1,3,3), stride=1),\n",
    "    nn.BatchNorm3d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=(3, 2, 2), stride=(1,2,2), padding=1),       \n",
    "    # conv 4-1\n",
    "    nn.Conv3d(32, 64, kernel_size=(1,3,3), stride=1),\n",
    "    nn.BatchNorm3d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=(3, 2, 2), stride=(1,2,2), padding=1),  \n",
    "    nn.Dropout(p=0.5),\n",
    "    # conv 4-2\n",
    "    nn.Conv3d(64, 64, kernel_size=(1,3,3), stride=1),\n",
    "    nn.BatchNorm3d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=(3, 2, 2), stride=(1,2,2), padding=1),\n",
    "    Flatten3d(),\n",
    "    nn.Linear(768,10),\n",
    "    #nn.ReLU(inplace=True),\n",
    "    #nn.Dropout(p=0.5),\n",
    "    #nn.Linear(64,10)\n",
    ")\n",
    "\n",
    "fixed_model_3d = fixed_model_3d_frame.type(dtype)\n",
    "x = torch.randn(32,3, 3, 64, 64).type(dtype)\n",
    "x_var = Variable(x).type(dtype) # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_3d(x_var) \n",
    "np.array_equal(np.array(ans.size()), np.array([32, 10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaolinghu/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 100, loss = 0.9059\n",
      "t = 200, loss = 1.2550\n",
      "t = 300, loss = 0.2361\n",
      "t = 400, loss = 0.1434\n",
      "Got 7166 / 7770 correct (92.23)\n",
      "Got 1448 / 2230 correct (64.93)\n",
      "Starting epoch 2 / 5\n",
      "t = 100, loss = 0.1000\n",
      "t = 200, loss = 0.3157\n",
      "t = 300, loss = 0.0326\n",
      "t = 400, loss = 0.0257\n",
      "Got 7679 / 7770 correct (98.83)\n",
      "Got 1432 / 2230 correct (64.22)\n",
      "Starting epoch 3 / 5\n",
      "t = 100, loss = 0.0439\n",
      "t = 200, loss = 0.0617\n",
      "t = 300, loss = 0.2620\n",
      "t = 400, loss = 0.0036\n",
      "Got 7741 / 7770 correct (99.63)\n",
      "Got 1435 / 2230 correct (64.35)\n",
      "Starting epoch 4 / 5\n",
      "t = 100, loss = 0.1341\n",
      "t = 200, loss = 0.0199\n",
      "t = 300, loss = 0.1130\n",
      "t = 400, loss = 0.0079\n",
      "Got 7763 / 7770 correct (99.91)\n",
      "Got 1496 / 2230 correct (67.09)\n",
      "Starting epoch 5 / 5\n",
      "t = 100, loss = 0.3267\n",
      "t = 200, loss = 0.0014\n",
      "t = 300, loss = 0.0087\n",
      "t = 400, loss = 0.0004\n",
      "Got 7768 / 7770 correct (99.97)\n",
      "Got 1535 / 2230 correct (68.83)\n",
      "Got 1535 / 2230 correct (68.83)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "\n",
    "fixed_model_gpu.apply(reset) \n",
    "fixed_model_gpu.train() \n",
    "train_3d(fixed_model_gpu, loss_fn, optimizer,clip_dataloader_train, num_epochs=5) \n",
    "check_accuracy_3d(fixed_model_gpu, clip_dataloader_val)# check accuracy on the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3270\n"
     ]
    }
   ],
   "source": [
    "def predict_on_test_3d(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    #model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('results_3d.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['clip'].type(dtype))\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i])+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "    \n",
    "count=predict_on_test_3d(fixed_model_gpu, clip_dataloader_test)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tingjin_jane/Downloads/resnet-18-kinetics.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7cb22efae6a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/tingjin_jane/Downloads/resnet-18-kinetics.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xiaolinghu/anaconda2/lib/python2.7/site-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/tingjin_jane/Downloads/resnet-18-kinetics.pth'"
     ]
    }
   ],
   "source": [
    "torch.load('./resnet-18-kinetics.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
