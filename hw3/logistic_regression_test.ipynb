{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin you first need to install all dependencies, using:\n",
    "\n",
    "    pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import exp, log\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from copy import deepcopy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### set your image dir here\n",
    "IMAGE_DIR = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(subset=\"train\"):\n",
    "    \"\"\"\n",
    "    1. subset = \"train\", \"val\", \"test\"\n",
    "    2. About the dataset: in \"train\",\"val\" subset, the first half of images are images of hands, the rest half are images of not-hand. \n",
    "    3. extract features from HoG\n",
    "    \"\"\"\n",
    "    path = os.path.join(IMAGE_DIR, subset)\n",
    "    name_list = os.listdir(path)\n",
    "    print(\"Number of images in {}-set: {}\".format(subset, len(name_list)))\n",
    "    #HoG returns 324 features\n",
    "    X = np.zeros(shape=(len(name_list), 324))\n",
    "\n",
    "    if subset == \"train\" or subset == \"val\":\n",
    "        #Make sure that we have equal number of positive and negative class images\n",
    "        assert len(name_list)%2 == 0\n",
    "        count = len(name_list)//2\n",
    "        y = np.array(count*[1] + count*[0])\n",
    "        for idx_true in range(count):\n",
    "            img_name = os.path.join(path,str(idx_true)+\".png\")\n",
    "            img = io.imread(img_name)\n",
    "            img = rgb2gray(img)\n",
    "            vec = hog(img)\n",
    "            X[idx_true, :] = vec\n",
    "        \n",
    "        for idx in range(count):\n",
    "            idx_false = idx + count\n",
    "            img_name = os.path.join(path,str(idx_false)+\".png\")\n",
    "            img = io.imread(img_name)\n",
    "            img = rgb2gray(img)\n",
    "            vec = hog(img)\n",
    "            X[idx_false, :] = vec\n",
    "        return X, y        \n",
    "    else:\n",
    "        for idx in range(len(name_list)):\n",
    "            img_name = os.path.join(path, str(idx)+\".png\")\n",
    "            img = io.imread(img_name)\n",
    "            img = rgb2gray(img)\n",
    "            vec = hog(img)\n",
    "            X[idx, :] = vec\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train-set: 8170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/skimage/feature/_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in val-set: 2724\n",
      "Number of images in test-set: 5542\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_dataset(\"train\")\n",
    "X_val, y_val = load_dataset(\"val\")\n",
    "X_test = load_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(img_path):\n",
    "    \"\"\"\n",
    "    visualize the img\n",
    "    \"\"\"\n",
    "    img = io.imread(img_path)\n",
    "    print(img_path)\n",
    "    io.imshow(img)\n",
    "    io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train/129.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c17e5bb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(IMAGE_DIR+\"train/129.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function in the class\n",
    "1. \\__init\\__() which sets the hyper parameters\n",
    "2. sigmoid()\n",
    "3. __init_param(), initializes the parameters .\n",
    "4. fit(): train the data\n",
    "5. predict_proba(): predict the probability of the sample belonging to the positive class\n",
    "6. predict(): classify the sample\n",
    "\n",
    "\n",
    "Feel free to overlook the rest of the parts of the code, it's just for convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, eta0=0.1, eta1=1, m=16, max_epoch=1000, delta=0.0001):\n",
    "        \"\"\"\n",
    "        m is the batch_size\n",
    "        \"\"\"\n",
    "        self.__init = True # whether to initial the parameters\n",
    "        self.__eta0 = eta0\n",
    "        self.__eta1 = eta1\n",
    "        self.__delta = delta\n",
    "        self.__m = m\n",
    "        self.__max_epoch = max_epoch\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + exp(-x))\n",
    "    \n",
    "    def __init_param(self):\n",
    "        \"\"\"\n",
    "        Weights initialized using a normal distribution here: you can change the distribution.\n",
    "        \"\"\"\n",
    "        d = self.__dimension\n",
    "        self.__wt = np.random.randn(1, d)\n",
    "        self.__bias = np.random.randn()\n",
    "        return self.__wt, self.__bias\n",
    "\n",
    "\n",
    "    def visual_wt(self):\n",
    "        '''visualize the weights\n",
    "        '''\n",
    "        pd.DataFrame(self.__wt[0]).hist(bins=15)\n",
    "\n",
    "\n",
    "    def create_batch(self, n, m):\n",
    "        '''Create batches of size m or m+1\n",
    "        Parameters:\n",
    "            n (int): the size of data set\n",
    "            m (int): the size of the batch can be either m or m+1\n",
    "        Return:\n",
    "            (list): a list of batches, where each batch is a list\n",
    "        '''\n",
    "        permute = np.random.permutation(n)\n",
    "        num_m_1 = n%m            # this number of batches have size of m+1\n",
    "        num_m = n//m - num_m_1   # this number of batches have size of m\n",
    "        batches = []\n",
    "        index = 0\n",
    "        for _ in range(num_m):\n",
    "            batch = []\n",
    "            for _ in range(m):\n",
    "                batch.append(permute[index])\n",
    "                index += 1\n",
    "            batches.append(batch)\n",
    "        for _ in range(num_m_1):\n",
    "            batch = []\n",
    "            for _ in range(m+1):\n",
    "                batch.append(permute[index])\n",
    "                index += 1\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "\n",
    "\n",
    "    def loss_function(self, X_bar, y, thetaT):\n",
    "        '''The loss function that we want to optimize\n",
    "        Parameters:\n",
    "            X_bar (ndarray, n=2): the training dataset\n",
    "            thetaT (ndarray, n=1): the weights\n",
    "        Return:\n",
    "            (float)\n",
    "        '''\n",
    "        summation = 0\n",
    "        for Xi, yi in zip(X_bar, y):\n",
    "            summation += yi * thetaT @ Xi - log(1 + exp(thetaT @ Xi))\n",
    "        return -summation/len(X_bar)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Recommended input:\n",
    "        X: n x d array,\n",
    "        y: n x 1 array or list\n",
    "        \"\"\"\n",
    "        n, d = X.shape                              # X.shape = (8170, 324)\n",
    "        self.__dimension = d\n",
    "\n",
    "        if self.__init:\n",
    "            self.__init_param()\n",
    "\n",
    "        ### write your code here ###\n",
    "        ones = np.array(np.ones(X.shape[0]))\n",
    "        ones = np.expand_dims(ones, axis=0)\n",
    "        X_bar = np.concatenate((X, ones.T), axis=1)  # X_bar.shape  = (8170, 325), i.e already transposed\n",
    "        thetaT = np.append(self.__wt[0], self.__bias)          # thetaT.shape = (325,)\n",
    "\n",
    "        for epoch in range(1, self.__max_epoch+1):\n",
    "            eta = self.__eta0/(self.__eta1 + epoch)  # eta decreases\n",
    "            batches = self.create_batch(n, self.__m)\n",
    "            new_thetaT = deepcopy(thetaT)\n",
    "            for batch in batches:                    # batch is a list of random index\n",
    "                derivative = np.zeros(len(thetaT))\n",
    "                for i in range(len(batch)):\n",
    "                    Xi = X_bar[batch[i]]\n",
    "                    yi = y[batch[i]]\n",
    "                    prob = self.sigmoid(new_thetaT @ Xi)\n",
    "                    derivative += (yi - prob) * Xi\n",
    "                derivative = -derivative/len(batch)             # take the average and times -1\n",
    "                new_thetaT = new_thetaT - eta*derivative          # update thetaT\n",
    "            loss_old = self.loss_function(X_bar, y, thetaT)\n",
    "            loss_new = self.loss_function(X_bar, y, new_thetaT)\n",
    "            print(epoch, loss_old, loss_new, loss_new/loss_old)\n",
    "            thetaT = deepcopy(new_thetaT)\n",
    "\n",
    "            if loss_new > (1 - self.__delta)*loss_old:   # terminate condition\n",
    "                self.__wt = thetaT[0:d]\n",
    "                self.__bias = thetaT[d]\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Classify the sample\n",
    "        \"\"\"\n",
    "        ones = np.array(np.ones(X.shape[0]))\n",
    "        ones = np.expand_dims(ones, axis=0)\n",
    "        X_bar = np.concatenate((X, ones.T), axis=1)         # X_bar.shape  = (8170, 325), i.e already transposed\n",
    "        thetaT = np.append(self.__wt, self.__bias)       # thetaT.shape = (325,) # TODO, need to change dimensions here\n",
    "        y_pred = []\n",
    "        for Xi in X_bar:\n",
    "            if self.sigmoid(thetaT @ Xi) >= 0.5:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def save_predicts(self, y_pred):\n",
    "        df_pred = pd.DataFrame(y_pred, columns=['Class'])\n",
    "        index = [i for i in range(1, len(df_pred)+1)]\n",
    "        df_pred.insert(0, 'Id', index)\n",
    "        df_pred.to_csv('data/predTestClass.csv ', header=True, index=False)\n",
    "\n",
    "\n",
    "    def get_param(self):\n",
    "        \"\"\"\n",
    "        output:\n",
    "            parameters: wt(1*d array), b(scalar)\n",
    "        \"\"\"\n",
    "        return [self.__wt, self.__bias]\n",
    "    \n",
    "    def save_model(self, save_file):\n",
    "        \"\"\"\n",
    "        save model to .pkl file\n",
    "        \"\"\"\n",
    "        with open(save_file,\"wb\") as file:\n",
    "            pickle.dump([self.__wt, self.__bias], file)\n",
    "\n",
    "    def load_model(self, load_file):\n",
    "        \"\"\"\n",
    "        load model from .pkl file\n",
    "        \"\"\"\n",
    "        with open(load_file,\"rb\") as file:\n",
    "            param = pickle.load(file)\n",
    "        self.__wt = param[0]\n",
    "        self.__bias = param[1]\n",
    "        self.__init = False\n",
    "        return self.__wt, self.__bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.744879389798 0.674117349418 0.905002015965\n",
      "2 0.674117349418 0.658053755049 0.976170922789\n",
      "3 0.658053755049 0.64684696367 0.982969793436\n",
      "4 0.64684696367 0.638321820409 0.986820463354\n",
      "5 0.638321820409 0.631549342727 0.989390183031\n",
      "6 0.631549342727 0.625962238969 0.991153337705\n",
      "7 0.625962238969 0.621207199652 0.992403632326\n",
      "8 0.621207199652 0.617103058997 0.993393282213\n",
      "9 0.617103058997 0.613493312449 0.994150496428\n",
      "10 0.613493312449 0.610284329998 0.99476932774\n",
      "11 0.610284329998 0.607394411485 0.995264635891\n",
      "12 0.607394411485 0.604776244439 0.995689510808\n",
      "13 0.604776244439 0.602383304387 0.996043263811\n",
      "14 0.602383304387 0.600184781556 0.996350292555\n",
      "15 0.600184781556 0.598149538287 0.996608972217\n",
      "16 0.598149538287 0.596259603816 0.996840364575\n",
      "17 0.596259603816 0.594496606059 0.99704323797\n",
      "18 0.594496606059 0.592845874536 0.997223312116\n",
      "19 0.592845874536 0.591293377831 0.997381281085\n",
      "20 0.591293377831 0.58983060977 0.997526155179\n",
      "21 0.58983060977 0.58844761 0.997655259412\n",
      "22 0.58844761 0.587136227857 0.997771454722\n",
      "23 0.587136227857 0.585891798396 0.997880509834\n",
      "24 0.585891798396 0.584705752964 0.997975657903\n",
      "25 0.584705752964 0.583574828932 0.998065823661\n",
      "26 0.583574828932 0.582494350648 0.998148518013\n",
      "27 0.582494350648 0.58146040083 0.998224961638\n",
      "28 0.58146040083 0.580469035782 0.998295042885\n",
      "29 0.580469035782 0.579517196587 0.998360223998\n",
      "30 0.579517196587 0.578601884759 0.998420561401\n",
      "31 0.578601884759 0.577721279254 0.998478045911\n",
      "32 0.577721279254 0.576872153803 0.998530216072\n",
      "33 0.576872153803 0.576053204998 0.998580363432\n",
      "34 0.576053204998 0.575262031455 0.998626561685\n",
      "35 0.575262031455 0.574497184859 0.998670437897\n",
      "36 0.574497184859 0.57375707974 0.998711734125\n",
      "37 0.57375707974 0.573040167558 0.998750495274\n",
      "38 0.573040167558 0.572345185548 0.998787201929\n",
      "39 0.572345185548 0.57167094473 0.998821968219\n",
      "40 0.57167094473 0.571016534123 0.998855266981\n",
      "41 0.571016534123 0.570380566223 0.998886253089\n",
      "42 0.570380566223 0.569762156248 0.998915794102\n",
      "43 0.569762156248 0.569160690958 0.998944357249\n",
      "44 0.569160690958 0.56857507026 0.99897108021\n",
      "45 0.56857507026 0.568004462745 0.998996425373\n",
      "46 0.568004462745 0.567448284849 0.999020821256\n",
      "47 0.567448284849 0.566906034551 0.999044405786\n",
      "48 0.566906034551 0.566376925855 0.999066673022\n",
      "49 0.566376925855 0.565860415548 0.999088044933\n",
      "50 0.565860415548 0.565355982843 0.999108556296\n",
      "51 0.565355982843 0.564862987299 0.999127990932\n",
      "52 0.564862987299 0.564381200918 0.999147073906\n",
      "53 0.564381200918 0.563910021008 0.999165138901\n",
      "54 0.563910021008 0.563449087763 0.99918261207\n",
      "55 0.563449087763 0.562997762264 0.999198995068\n",
      "56 0.562997762264 0.562555913035 0.99921518475\n",
      "57 0.562555913035 0.562123141689 0.999230705186\n",
      "58 0.562123141689 0.561699090553 0.999245625905\n",
      "59 0.561699090553 0.561283404308 0.999259948517\n",
      "60 0.561283404308 0.560875841114 0.999273872716\n",
      "61 0.560875841114 0.560475955771 0.999287034111\n",
      "62 0.560475955771 0.560083625132 0.999300004514\n",
      "63 0.560083625132 0.559698643752 0.999312635893\n",
      "64 0.559698643752 0.559320709574 0.999324754165\n",
      "65 0.559320709574 0.558949505362 0.999336330293\n",
      "66 0.558949505362 0.558584879494 0.999347658661\n",
      "67 0.558584879494 0.558226625088 0.999358639271\n",
      "68 0.558226625088 0.55787452443 0.999369251408\n",
      "69 0.55787452443 0.557528419352 0.999379600497\n",
      "70 0.557528419352 0.557188033301 0.999389473183\n",
      "71 0.557188033301 0.556853324554 0.99939928942\n",
      "72 0.556853324554 0.556523968092 0.99940854001\n",
      "73 0.556523968092 0.556199910646 0.999417711608\n",
      "74 0.556199910646 0.555880999933 0.999426625737\n",
      "75 0.555880999933 0.55556703039 0.999435185691\n",
      "76 0.55556703039 0.555257879268 0.999443539474\n",
      "77 0.555257879268 0.554953444074 0.999451722874\n",
      "78 0.554953444074 0.55465356989 0.999459640828\n",
      "79 0.55465356989 0.554358115201 0.999467316709\n",
      "80 0.554358115201 0.554066995765 0.999474853117\n",
      "81 0.554066995765 0.553780053117 0.99948211561\n",
      "82 0.553780053117 0.55349718646 0.999489207573\n",
      "83 0.55349718646 0.553218253207 0.999496052989\n",
      "84 0.553218253207 0.552943247382 0.999502898136\n",
      "85 0.552943247382 0.552672035801 0.999509512808\n",
      "86 0.552672035801 0.55240449622 0.999515916197\n",
      "87 0.55240449622 0.552140551428 0.999522189276\n",
      "88 0.552140551428 0.551880093739 0.999528276471\n",
      "89 0.551880093739 0.551623121156 0.999534368814\n",
      "90 0.551623121156 0.551369439121 0.99954011711\n",
      "91 0.551369439121 0.55111901284 0.999545810371\n",
      "92 0.55111901284 0.550871811145 0.999551454968\n",
      "93 0.550871811145 0.55062769122 0.999556848036\n",
      "94 0.55062769122 0.550386630543 0.999562207494\n",
      "95 0.550386630543 0.55014851867 0.999567373444\n",
      "96 0.55014851867 0.549913258358 0.999572369452\n",
      "97 0.549913258358 0.549680847808 0.99957736871\n",
      "98 0.549680847808 0.549451211338 0.999582236726\n",
      "99 0.549451211338 0.549224263417 0.999586955281\n",
      "100 0.549224263417 0.548999988792 0.999591652008\n",
      "101 0.548999988792 0.548778255236 0.999596113734\n",
      "102 0.548778255236 0.548559118919 0.999600683309\n",
      "103 0.548559118919 0.548342465151 0.999605049373\n",
      "104 0.548342465151 0.54812823309 0.999609309738\n",
      "105 0.54812823309 0.547916366342 0.999613472294\n",
      "106 0.547916366342 0.547706887234 0.999617680506\n",
      "107 0.547706887234 0.547499671729 0.999621667156\n",
      "108 0.547499671729 0.547294710574 0.9996256415\n",
      "109 0.547294710574 0.547091896895 0.999629425107\n",
      "110 0.547091896895 0.546891262503 0.999633271133\n",
      "111 0.546891262503 0.546692729698 0.999636979379\n",
      "112 0.546692729698 0.546496284591 0.999640666326\n",
      "113 0.546496284591 0.546301911095 0.999644327874\n",
      "114 0.546301911095 0.546109481109 0.999647758901\n",
      "115 0.546109481109 0.545919008814 0.999651219578\n",
      "116 0.545919008814 0.545730423113 0.999654553701\n",
      "117 0.545730423113 0.545543756549 0.999657950965\n",
      "118 0.545543756549 0.545358891216 0.999661135645\n",
      "119 0.545358891216 0.545175852098 0.999664369426\n",
      "120 0.545175852098 0.544994620431 0.999667572094\n",
      "121 0.544994620431 0.5448151267 0.999670650453\n",
      "122 0.5448151267 0.544637321326 0.9996736409\n",
      "123 0.544637321326 0.544461274791 0.999676763733\n",
      "124 0.544461274791 0.544286822011 0.999679586432\n",
      "125 0.544286822011 0.544114028695 0.999682532611\n",
      "126 0.544114028695 0.543942811379 0.999685328246\n",
      "127 0.543942811379 0.543773177206 0.999688139691\n",
      "128 0.543773177206 0.543605101941 0.99969090924\n",
      "129 0.543605101941 0.543438549041 0.999693614171\n",
      "130 0.543438549041 0.543273471412 0.999696234966\n",
      "131 0.543273471412 0.543109870932 0.999698861645\n",
      "132 0.543109870932 0.542947726715 0.999701452274\n",
      "133 0.542947726715 0.542787008161 0.999703988898\n",
      "134 0.542787008161 0.542627640408 0.999706389891\n",
      "135 0.542627640408 0.542469695657 0.999708926087\n",
      "136 0.542469695657 0.542313104268 0.999711336154\n",
      "137 0.542313104268 0.542157839717 0.99971369943\n",
      "138 0.542157839717 0.542003875744 0.999716016329\n",
      "139 0.542003875744 0.541851197451 0.999718307746\n",
      "140 0.541851197451 0.541699812467 0.999720615208\n",
      "141 0.541699812467 0.541549659581 0.999722811635\n",
      "142 0.541549659581 0.541400771024 0.999725069429\n",
      "143 0.541400771024 0.541253083332 0.999727211893\n",
      "144 0.541253083332 0.541106592225 0.999729348226\n",
      "145 0.541106592225 0.540961295075 0.999731481464\n",
      "146 0.540961295075 0.540817167767 0.999733571866\n",
      "147 0.540817167767 0.540674134302 0.999735523439\n",
      "148 0.540674134302 0.54053228129 0.99973763677\n",
      "149 0.54053228129 0.540391528562 0.999739603474\n",
      "150 0.540391528562 0.540251875631 0.999741570837\n",
      "151 0.540251875631 0.540113276751 0.999743455069\n",
      "152 0.540113276751 0.539975747169 0.999745369002\n",
      "153 0.539975747169 0.539839258088 0.99974723109\n",
      "154 0.539839258088 0.539703804256 0.999749084881\n",
      "155 0.539703804256 0.539569375237 0.999750920749\n",
      "156 0.539569375237 0.539435951711 0.999752722205\n",
      "157 0.539435951711 0.539303523454 0.999754506059\n",
      "158 0.539303523454 0.539172056659 0.999756228564\n",
      "159 0.539172056659 0.53904155046 0.999757950736\n",
      "160 0.53904155046 0.538911997589 0.999759660697\n",
      "161 0.538911997589 0.538783391497 0.999761359752\n",
      "162 0.538783391497 0.538655717923 0.999763033577\n",
      "163 0.538655717923 0.538528953384 0.999764665007\n",
      "164 0.538528953384 0.538403090974 0.999766284785\n",
      "165 0.538403090974 0.53827811524 0.999767877014\n",
      "166 0.53827811524 0.538154022207 0.999769462979\n",
      "167 0.538154022207 0.538030784297 0.999770998813\n",
      "168 0.538030784297 0.537908403102 0.999772538676\n",
      "169 0.537908403102 0.537786856671 0.999774038794\n",
      "170 0.537786856671 0.537666174378 0.999775594567\n",
      "171 0.537666174378 0.537546290907 0.999777029918\n",
      "172 0.537546290907 0.537427230733 0.999778511774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 0.537427230733 0.537308956429 0.999779924989\n",
      "174 0.537308956429 0.537191483025 0.999781367122\n",
      "175 0.537191483025 0.537074801511 0.99978279344\n",
      "176 0.537074801511 0.536958894442 0.99978418822\n",
      "177 0.536958894442 0.536843719988 0.999785506014\n",
      "178 0.536843719988 0.53672932312 0.999786908435\n",
      "179 0.53672932312 0.53661564683 0.999788205553\n",
      "180 0.53661564683 0.536502728865 0.999789573849\n",
      "181 0.536502728865 0.536390539936 0.999790888428\n",
      "182 0.536390539936 0.536279049671 0.99979214722\n",
      "183 0.536279049671 0.53616829146 0.999793469071\n",
      "184 0.53616829146 0.536058204388 0.999794678137\n",
      "185 0.536058204388 0.535948817523 0.999795942187\n",
      "186 0.535948817523 0.535840151955 0.99979724637\n",
      "187 0.535840151955 0.535732137586 0.99979842054\n",
      "188 0.535732137586 0.535624785502 0.99979961612\n",
      "189 0.535624785502 0.535518090891 0.999800803446\n",
      "190 0.535518090891 0.535412068853 0.999802019691\n",
      "191 0.535412068853 0.535306691231 0.999803184073\n",
      "192 0.535306691231 0.535201943428 0.999804321889\n",
      "193 0.535201943428 0.535097836396 0.999805480841\n",
      "194 0.535097836396 0.534994355017 0.999806612226\n",
      "195 0.534994355017 0.534891491845 0.999807730361\n",
      "196 0.534891491845 0.534789234318 0.999808825662\n",
      "197 0.534789234318 0.534687576959 0.999809911358\n",
      "198 0.534687576959 0.534586522808 0.999811003369\n",
      "199 0.534586522808 0.534486046821 0.999812049157\n",
      "200 0.534486046821 0.534386158972 0.999813114207\n",
      "201 0.534386158972 0.534286845287 0.99981415371\n",
      "202 0.534286845287 0.534188114398 0.999815209957\n",
      "203 0.534188114398 0.534089942506 0.999816222246\n",
      "204 0.534089942506 0.533992322098 0.999817221032\n",
      "205 0.533992322098 0.533895281084 0.999818272643\n",
      "206 0.533895281084 0.533798785615 0.999819261431\n",
      "207 0.533798785615 0.533702833126 0.999820245959\n",
      "208 0.533702833126 0.533607408307 0.999821202338\n",
      "209 0.533607408307 0.533512510979 0.9998221589\n",
      "210 0.533512510979 0.533418162335 0.999823155705\n",
      "211 0.533418162335 0.533324319047 0.999824071816\n",
      "212 0.533324319047 0.533231010404 0.999825043338\n",
      "213 0.533231010404 0.533138193503 0.999825934916\n",
      "214 0.533138193503 0.533045867985 0.999826826292\n",
      "215 0.533045867985 0.532954063438 0.999827773644\n",
      "216 0.532954063438 0.532862746158 0.999828658254\n",
      "217 0.532862746158 0.532771914126 0.999829539534\n",
      "218 0.532771914126 0.532681575397 0.99983043639\n",
      "219 0.532681575397 0.5325917048 0.999831286454\n",
      "220 0.5325917048 0.532502327899 0.999832184955\n",
      "221 0.532502327899 0.532413403213 0.999833006014\n",
      "222 0.532413403213 0.532324943217 0.999833850922\n",
      "223 0.532324943217 0.532236952829 0.999834705495\n",
      "224 0.532236952829 0.532149392444 0.999835486086\n",
      "225 0.532149392444 0.532062304074 0.999836346011\n",
      "226 0.532062304074 0.531975671649 0.999837176166\n",
      "227 0.531975671649 0.531889479813 0.99983797786\n",
      "228 0.531889479813 0.531803734988 0.999838792027\n",
      "229 0.531803734988 0.531718409213 0.999839554015\n",
      "230 0.531718409213 0.531633527499 0.999840363409\n",
      "231 0.531633527499 0.531549083033 0.999841160383\n",
      "232 0.531549083033 0.531465067257 0.99984194164\n",
      "233 0.531465067257 0.531381455177 0.999842676244\n",
      "234 0.531381455177 0.531298253075 0.999843423023\n",
      "235 0.531298253075 0.531215459657 0.999844167721\n",
      "236 0.531215459657 0.531133089742 0.999844940667\n",
      "237 0.531133089742 0.531051113352 0.999845657534\n",
      "238 0.531051113352 0.530969544621 0.999846401357\n",
      "239 0.530969544621 0.530888367839 0.999847115935\n",
      "240 0.530888367839 0.530807580466 0.999847826064\n",
      "241 0.530807580466 0.530727198428 0.999848566523\n",
      "242 0.530727198428 0.530647197497 0.99984926167\n",
      "243 0.530647197497 0.530567567545 0.999849938053\n",
      "244 0.530567567545 0.530488326909 0.999850649303\n",
      "245 0.530488326909 0.530409440322 0.999851294397\n",
      "246 0.530409440322 0.530330956568 0.999852031755\n",
      "247 0.530330956568 0.530252822307 0.999852668866\n",
      "248 0.530252822307 0.530175059967 0.99985334856\n",
      "249 0.530175059967 0.530097657531 0.999854005891\n",
      "250 0.530097657531 0.530020604421 0.999854643557\n",
      "251 0.530020604421 0.529943929943 0.999855336797\n",
      "252 0.529943929943 0.529867615492 0.999855995235\n",
      "253 0.529867615492 0.529791635829 0.999856606329\n",
      "254 0.529791635829 0.529716002923 0.999857240278\n",
      "255 0.529716002923 0.529640719425 0.99985787951\n",
      "256 0.529640719425 0.529565782118 0.999858512942\n",
      "257 0.529565782118 0.529491172317 0.999859111364\n",
      "258 0.529491172317 0.529416891031 0.999859711946\n",
      "259 0.529416891031 0.529342958434 0.999860350891\n",
      "260 0.529342958434 0.529269360153 0.999860962954\n",
      "261 0.529269360153 0.52919607858 0.999861542009\n",
      "262 0.52919607858 0.529123123239 0.999862139301\n",
      "263 0.529123123239 0.529050491283 0.999862731466\n",
      "264 0.529050491283 0.528978175219 0.999863309711\n",
      "265 0.528978175219 0.528906188369 0.999863913384\n",
      "266 0.528906188369 0.528834507879 0.999864474094\n",
      "267 0.528834507879 0.528763136659 0.999865040539\n",
      "268 0.528763136659 0.528692083451 0.999865623749\n",
      "269 0.528692083451 0.528621335554 0.999866183174\n",
      "270 0.528621335554 0.528550893327 0.999866743503\n",
      "271 0.528550893327 0.528480752488 0.999867295961\n",
      "272 0.528480752488 0.528410905733 0.999867834816\n",
      "273 0.528410905733 0.52834134788 0.999868364085\n",
      "274 0.52834134788 0.528272093418 0.999868920988\n",
      "275 0.528272093418 0.528203123537 0.999869442506\n",
      "276 0.528203123537 0.52813444014 0.999869967833\n",
      "277 0.52813444014 0.528066058809 0.99987052287\n",
      "278 0.528066058809 0.527997961348 0.999871043669\n",
      "279 0.527997961348 0.527930142767 0.999871555222\n",
      "280 0.527930142767 0.527862612612 0.999872085055\n",
      "281 0.527862612612 0.527795357319 0.999872589399\n",
      "282 0.527795357319 0.527728384982 0.999873109271\n",
      "283 0.527728384982 0.52766168276 0.999873605015\n",
      "284 0.52766168276 0.527595260709 0.999874120002\n",
      "285 0.527595260709 0.527529093974 0.999874588079\n",
      "286 0.527529093974 0.527463195767 0.99987508138\n",
      "287 0.527463195767 0.527397569227 0.999875580815\n",
      "288 0.527397569227 0.527332218551 0.999876088401\n",
      "289 0.527332218551 0.527267119711 0.999876550613\n",
      "290 0.527267119711 0.527202290081 0.999877045945\n",
      "291 0.527202290081 0.527137723904 0.999877530545\n",
      "292 0.527137723904 0.527073387699 0.999877951811\n",
      "293 0.527073387699 0.527009329222 0.999878463837\n",
      "294 0.527009329222 0.526945500695 0.999878885395\n",
      "295 0.526945500695 0.526881935553 0.999879370556\n",
      "296 0.526881935553 0.526818614001 0.999879818329\n",
      "297 0.526818614001 0.526755555661 0.999880303507\n",
      "298 0.526755555661 0.526692738127 0.999880746329\n",
      "299 0.526692738127 0.526630165624 0.99988119733\n",
      "300 0.526630165624 0.526567833893 0.999881640409\n",
      "301 0.526567833893 0.526505749296 0.999882095729\n",
      "302 0.526505749296 0.526443882152 0.999882494837\n",
      "303 0.526443882152 0.526382268075 0.999882961737\n",
      "304 0.526382268075 0.526320888307 0.999883393171\n",
      "305 0.526320888307 0.526259725431 0.999883791661\n",
      "306 0.526259725431 0.526198812117 0.999884252374\n",
      "307 0.526198812117 0.526138120559 0.999884660405\n",
      "308 0.526138120559 0.526077668711 0.999885102702\n",
      "309 0.526077668711 0.52601743692 0.999885507798\n",
      "310 0.52601743692 0.525957437271 0.999885936008\n",
      "311 0.525957437271 0.52589764452 0.999886316368\n",
      "312 0.52589764452 0.525838091988 0.999886760224\n",
      "313 0.525838091988 0.525778763558 0.999887173578\n",
      "314 0.525778763558 0.525719648631 0.999887566917\n",
      "315 0.525719648631 0.525660752429 0.999887970324\n",
      "316 0.525660752429 0.525602072193 0.999888368618\n",
      "317 0.525602072193 0.525543610926 0.999888772763\n",
      "318 0.525543610926 0.52548535977 0.999889160185\n",
      "319 0.52548535977 0.525427315705 0.999889541995\n",
      "320 0.525427315705 0.525369479143 0.999889924715\n",
      "321 0.525369479143 0.525311860783 0.999890327926\n",
      "322 0.525311860783 0.525254436324 0.999890685014\n",
      "323 0.525254436324 0.52519722833 0.999891085177\n",
      "324 0.52519722833 0.525140208664 0.999891431898\n",
      "325 0.525140208664 0.525083404713 0.999891830887\n",
      "326 0.525083404713 0.525026806796 0.999892211568\n",
      "327 0.525026806796 0.524970409337 0.999892581753\n",
      "328 0.524970409337 0.52491420041 0.999892929343\n",
      "329 0.52491420041 0.524858188342 0.999893292908\n",
      "330 0.524858188342 0.524802383181 0.999893675734\n",
      "331 0.524802383181 0.524746786667 0.999894062003\n",
      "332 0.524746786667 0.524691368617 0.999894390873\n",
      "333 0.524691368617 0.524636142008 0.999894744581\n",
      "334 0.524636142008 0.524581111111 0.999895106548\n",
      "335 0.524581111111 0.524526272566 0.999895462219\n",
      "336 0.524526272566 0.524471620556 0.999895806915\n",
      "337 0.524471620556 0.524417147039 0.999896136388\n",
      "338 0.524417147039 0.524362860545 0.999896482231\n",
      "339 0.524362860545 0.52430876219 0.999896830307\n",
      "340 0.52430876219 0.524254858526 0.999897190992\n",
      "341 0.524254858526 0.524201134484 0.999897523044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 0.524201134484 0.524147584299 0.999897844202\n",
      "343 0.524147584299 0.524094218772 0.999898186067\n",
      "344 0.524094218772 0.524041043817 0.999898539321\n",
      "345 0.524041043817 0.523988040057 0.999898855709\n",
      "346 0.523988040057 0.523935213101 0.999899182898\n",
      "347 0.523935213101 0.523882564411 0.999899512977\n",
      "348 0.523882564411 0.523830083864 0.999899823833\n",
      "349 0.523830083864 0.523777780948 0.999900152897\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.save_predicts(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicts(self, y_pred):\n",
    "    df_pred = pd.DataFrame(y_pred, columns=['Class'])\n",
    "    index = [i for i in range(0, len(df_pred))]\n",
    "    df_pred.insert(0, 'Id', index)\n",
    "    df_pred.to_csv('data/predTestClass.csv ', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_predicts() missing 1 required positional argument: 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-371-1ae399d69316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_predicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save_predicts() missing 1 required positional argument: 'y_pred'"
     ]
    }
   ],
   "source": [
    "save_predicts(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 325)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5542, 324)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Number of epochs till termination = __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (b) Plot L(theta) vs num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Final value of L($\\theta$) after optimization = __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experimenting with ($\\eta_0$,$\\eta_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experimentation code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Best value for, $\\eta_0$ = \\__, $\\eta_1$ = \\__\n",
    "\n",
    "Number of epochs for training = __\n",
    "\n",
    "Final value of L($\\theta$) = __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (b) For best (eta_0, eta_1) plot L(theta) vs num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluating on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (a) Plot L(theta) vs num_epochs for both training and validation set on the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (b) Plot Accuracy vs num_epochs for both training and validation set on the same figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ROC and Precision-Recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(a) Plot ROC curve on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under curve = __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(a) Plot Precision-Recall curve on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Average Precision = __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def submission(y_pred):\n",
    "    \"\"\"\n",
    "    generate the submission.csv for kaggle\n",
    "    input args:\n",
    "        y_pred: 5542x1 array or list contains 5542 elements\n",
    "        \n",
    "    \"\"\"\n",
    "    save_file = pd.DataFrame(columns=[\"Id\",\"Class\"])\n",
    "    id = np.array([idx for idx in range(5542)])\n",
    "    save_file[\"Id\"] = id\n",
    "    save_file[\"Class\"]= y_pred\n",
    "    save_file.to_csv(\"submission.csv\",index=0)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best obtained accuracy on Public Leader-board = __%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
